---
title: "Capstone Project"
author: "Jeevithiesh Duggani"
date: "11/05/2020"
output: 
    html_document:
        toc: true
---
```{r setup, include=FALSE}
    options(scipen = 10)
    setwd("C:\\Users\\Jeevithiesh\\Desktop\\Learning\\Coursera\\Data Science Specialization\\Capstone Project")
```

## Abstract

## Import the necessary libraries
Import the following libraries.

- stringi
- tm
- ggplot2
```{r import libraries, message=FALSE, results='hide'}
    library(stringi)
    library(tm)
    library(RWeka)
    library(SnowballC)
    library(ggplot2)
```

## Import the Data into the Environment

### Download the Dataset
This project uses the `dataset` provided by the **Data Science Specialization Course** on Coursera. The dataset can be accessed from [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

Download and Extract the `dataset` if it is not already present `r getwd()`.
```{r download and extract the dataset, results='hide'}
    url     <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    zipFile <- "Coursera-SwiftKey.zip"
    dFolder <- "final"
    if(!(file.exists(zipFile) | file.exists(dFolder))) download.file(url, zipFile) 
    if(!file.exists(dFolder)) unzip(zipFile)
    if(file.exists(zipFile)) file.remove(zipFile)
```

### Understanding the data of the file
Size of `en_US.blogs.txt`.
```{bash size of file}
    ls -alh "final/en_US/en_US.blogs.txt"
```

Number of lines in `en_US.twitter.txt`.
```{bash lines in file}
    wc -l "final/en_US/en_US.twitter.txt"
```

Longest of lines in `en_US`.
```{bash longest lines in file}
    wc -L "final/en_US/en_US.blogs.txt" "final/en_US/en_US.news.txt" "final/en_US/en_US.twitter.txt"
```

(lines with *love*) / (lines with *hate*) in `en_US.twitter.txt`.
```{bash love / hate}
    n=$(grep -c "love" ./final/en_US/en_US.twitter.txt)
    d=$(grep -c "hate" ./final/en_US/en_US.twitter.txt)
    expr $((n / d))
```

Tweet with *biostats* in `en_US.twitter.txt`.
```{bash tweet with biostats}
   grep -a "biostats" final/en_US/en_US.twitter.txt
```

Tweet in `en_US.twitter.txt`.
```{bash match tweet}
   grep -c "^A computer once beat me at chess, but it was no match for me at kickboxing$" final/en_US/en_US.twitter.txt
```

### Import the data from the dataset
The datasets consists of data collecteed from **three** different sources in **four** countries. Store the data in individual `data frames` seperated by **source** and **countries**.
```{r store the data in dataframes, warning=FALSE}
    ##  Datasets sourced from blogs
    DEBlogData <- readLines("final/de_DE/de_DE.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
    USBlogData <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
    FIBlogData <- readLines("final/fi_FI/fi_FI.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
    RUBlogData <- readLines("final/ru_RU/ru_RU.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
    ##  Datasets sourced from News
    DENewsData <- readLines("final/de_DE/de_DE.news.txt", encoding = "UTF-8", skipNul = TRUE)
    USNewsData <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
    FINewsData <- readLines("final/fi_FI/fi_FI.news.txt", encoding = "UTF-8", skipNul = TRUE)
    RUNewsData <- readLines("final/ru_RU/ru_RU.news.txt", encoding = "UTF-8", skipNul = TRUE)
    ##  Datasets sourced from blogs
    DETwitterData <- readLines("final/de_DE/de_DE.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
    USTwitterData <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
    FITwitterData <- readLines("final/fi_FI/fi_FI.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
    RUTwitterData <- readLines("final/ru_RU/ru_RU.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

## Identifying the Data
Identify the number of lines and words
```{r number of lines in the Blog datasets}
    length(DEBlogData);length(USBlogData);length(FIBlogData);length(RUBlogData)
```
```{r number of lines in the news datasets}
    length(DENewsData);length(USNewsData);length(FINewsData);length(RUNewsData)
```
```{r number of lines in the twitter datasets}
    length(DETwitterData);length(USTwitterData);length(FITwitterData);length(RUTwitterData)
```

- There are 181958, 244743, 947774 lines in Blog, News and Twitter datasets in German
- There are 899288, 77259, 2360148 lines in Blog, News and Twitter datasets in English, US.
- There are 439785, 485758, 285214 lines in Blog, News and Twitter datasets in Finnish
- There are 337100, 196360, 881414 lines in Blog, News and Twitter datasets in Russian

```{r number of words in the Blog datasets}
    sum(stri_count_words(DEBlogData));
    sum(stri_count_words(USBlogData));
    sum(stri_count_words(FIBlogData));
    sum(stri_count_words(RUBlogData))
```

```{r number of words in the News datasets}
    sum(stri_count_words(DENewsData));
    sum(stri_count_words(USNewsData));
    sum(stri_count_words(FINewsData));
    sum(stri_count_words(RUNewsData))
```

```{r number of words in the Twitter datasets}
    sum(stri_count_words(DETwitterData));
    sum(stri_count_words(USTwitterData));
    sum(stri_count_words(FITwitterData));
    sum(stri_count_words(RUTwitterData))
```

- There are 6205913,  13375092, 11646047 words in Blog, News and Twitter datasets in German
- There are 37546239,  2674536, 30093413 words in Blog, News and Twitter datasets in English, US.
- There are 12785318, 10532432, 3147077  words in Blog, News and Twitter datasets in Finnish
- There are 9388482,   9057248, 9231336  words in Blog, News and Twitter datasets in Russian

## Cleaning the Data
Remove URLs, punctuations, special characters, excess whitespace, stopwords and different cases. The following part of the assignment will be only for the data in the US Database. The preprocessing steps will be demonstrated on `1%` of the data provided.
``` {r take the sample from the given datasets}
    set.seed(2020)

    USBlogSample <- sample(USBlogData, length(USBlogData) * 0.01)
    USNewsSample <- sample(USNewsData, length(USNewsData) * 0.01)
    USTwitterSample <- sample(USTwitterData, length(USTwitterData) * 0.01)
    
    USBlogSample    <- iconv(USBlogSample,    "latin1", "ASCII", sub="")
    USNewsSample    <- iconv(USNewsSample,    "latin1", "ASCII", sub="")
    USTwitterSample <- iconv(USTwitterSample, "latin1", "ASCII", sub="")

    sampleData <- c(USBlogSample, USNewsSample, USTwitterSample)
```

```{r helper function}
    Textprocessing <- function(x){
        gsub("http[[:alnum:]]*",'', x)
        gsub('http\\S+\\s*', '', x) ## Remove URLs
        gsub('\\b+RT', '', x) ## Remove RT
        gsub('#\\S+', '', x) ## Remove Hashtags
        gsub('@\\S+', '', x) ## Remove Mentions
        gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
        gsub("\\d", '', x) ## Remove Controls and special characters
        gsub('[[:punct:]]', '', x) ## Remove Punctuations
        gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
        gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
        gsub(' +',' ',x) ## Remove extra whitespaces
    }
```

```{r cleaning the data, warning=FALSE, cache=TRUE}
    corpus <- Corpus(VectorSource(sampleData))
    
    corpus <- tm_map(corpus, Textprocessing)
    corpus <- tm_map(corpus, tolower)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeWords, stopwords("en"))
    corpus <- tm_map(corpus, stripWhitespace)
    
    corpus <- VCorpus(VectorSource(corpus), readerControl=list(reader=readPlain, language="en"))
```

## Graphical Analysis
Plot the frequency histograms of the unigrams.
```{r helper function to calculate frequency}
    freq_table <- function(nCorpus){
        freq <- sort(rowSums(as.matrix(nCorpus)), decreasing = TRUE)
        freq_table <- data.frame(nGram = names(freq), freq = freq)
        return(freq_table)
    }
```

```{r histograms of the unigram, cache=TRUE}
    uniCorpus <- TermDocumentMatrix(corpus)
    uniCorpus <- removeSparseTerms(uniCorpus, 0.99)
    unigrams  <- freq_table(uniCorpus)
    plotData  <- head(unigrams, 20)
    
    plot <- ggplot(plotData, aes(x = reorder(nGram, freq), y = freq, fill = freq))
    plot <- plot + geom_bar(stat="identity")
    plot <- plot + theme_bw() + coord_flip() + theme(axis.title.y = element_blank())
    plot <- plot + labs(y="Frequency", title="Most common unigrams in text sample")
    plot
```

```{r histograms of the bigram, cache=TRUE}
    bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
    biCorpus <- TermDocumentMatrix(corpus, control = list(tokenize = bigramTokenizer))
    biCorpus <- removeSparseTerms(biCorpus, 0.999)
    bigrams  <- freq_table(biCorpus)
    plotData  <- head(bigrams, 20)
    
    plot <- ggplot(plotData, aes(x = reorder(nGram, freq), y = freq, fill = freq))
    plot <- plot + geom_bar(stat="identity")
    plot <- plot + theme_bw() + coord_flip() + theme(axis.title.y = element_blank())
    plot <- plot + labs(y="Frequency", title="Most common bigrams in text sample")
    plot
```

```{r histograms of the trigram, cache=TRUE}
    trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
    triCorpus <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
    triCorpus <- removeSparseTerms(triCorpus, 0.9999)
    trigrams  <- freq_table(triCorpus)
    plotData  <- head(trigrams, 20)
    
    plot <- ggplot(plotData, aes(x = reorder(nGram, freq), y = freq, fill = freq))
    plot <- plot + geom_bar(stat="identity")
    plot <- plot + theme_bw() + coord_flip() + theme(axis.title.y = element_blank())
    plot <- plot + labs(y="Frequency", title="Most common bigrams in text sample")
    plot
```

## Next Steps
The next step is to create a *model* and integrate it into a **Shiny app** for word prediction.

### Model
- Use the Ngram dataframes created to calculate the probabilities of the next word. 
- Tokenize the input string
- Cross Check the `quadgrams`, `trigrams`, `bigrams` and `unigrams` in the end of the input string with the Ngram dataframes in order to identify the next word.

### Shiny App
- Integrate the model framework created above into a shiny application.
- Generate the Shiny App.










